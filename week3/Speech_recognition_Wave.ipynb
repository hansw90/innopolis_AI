{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"Speech_recognition_Wave.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"s0hOgPd6phMx","colab_type":"text"},"source":["# Simple speech recognition\n","\n","Audio 데이터를 다뤄서 학습하는 방법을 배워보도록 합시다.\n","머신러닝 작업과정은 아래와 같습니다.\n","\n","1. Examine and understand data\n","2. Build an input pipeline\n","3. Build the model\n","4. Train the model\n","5. Test the model\n","6. Improve the model and repeat the process\n","\n","* 모델 완성 후 평가 지표에 따라서 모델을 평가해 봅시다.\n","* tensorflow 2.1 버전에 최적화되어있습니다."]},{"cell_type":"markdown","metadata":{"id":"ZglPODWg4J9S","colab_type":"text"},"source":["## Project 설명\n","### Task\n","* 1초 길이의 오디오 음성데이터를 이용해 단어를 분류하는 것이 목표입니다.\n","* 주어진 데이터를 이용해 딥러닝 트레이닝 과정을 구현해 보는것이 목표입니다.\n","* This code is borrowed from [Kaggle/TensorFlow Speech Recognition Challenge](https://www.kaggle.com/c/tensorflow-speech-recognition-challenge).\n","* This is version 0.01 of the data set containing 64,727 audio files, released on August 3rd 2017.\n","\n","### Baseline\n","* 기본적으로 사용하는 Convulution layers를 구성해 사용해보자.\n","    * ex) Conv - Conv - pooling - FC layers\n","* 오버피팅을 방지하기 위한 다양한 방법들을 사용해보자.\n","* Training\n","    * tf.data.dataset과 model.fit()을 사용\n","* Evaluation\n","    * 모델의 정확도와 크기를 이용해 점수를 제공하는 메트릭으로 평가해보자."]},{"cell_type":"markdown","metadata":{"id":"ErNtfQZV2J51","colab_type":"text"},"source":["### Setting tensorflow version"]},{"cell_type":"code","metadata":{"id":"LVezAn-epit5","colab_type":"code","colab":{}},"source":["!pip install tensorflow-gpu==2.1"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1I3lhvzF4rKF","colab_type":"text"},"source":["### Import packages\n","\n","* 우리가 사용할 packages 를 import 하는 부분 입니다.\n","* 필요에 따른 packages를 선언합니다."]},{"cell_type":"code","metadata":{"id":"GVi6R2-_phMz","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","from tensorflow.keras import layers\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","import librosa\n","import librosa.display\n","import matplotlib.pyplot as plt\n","\n","import os\n","from os.path import isdir, join\n","\n","import random\n","import copy\n","\n","tf.__version__"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e8pXqNOekZf3","colab_type":"text"},"source":["## Import modules\n","\n","### Import colab modules for Google Colab (if necessary)"]},{"cell_type":"code","metadata":{"id":"wARwEZj9p18i","colab_type":"code","colab":{}},"source":["use_colab = True\n","assert use_colab in [True, False]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zW_hpqTW2kTD","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-ADXEW7_RuF_","colab_type":"code","colab":{}},"source":["# Unfortunately you cannot downlaod GIANA dataset from website\n","# So I upload zip file on my dropbox\n","# if you want to download from my dropbox uncomment below  \n","if use_colab:\n","    DATASET_PATH='./gdrive/My Drive/datasets'\n","else:\n","    DATASET_PATH='./datasets'\n","\n","if not os.path.isdir(DATASET_PATH):\n","    os.makedirs(DATASET_PATH)\n","\n","    import urllib.request\n","    u = urllib.request.urlopen(url='http://bigfile.mail.naver.com/bigfileupload/download?fid=8XYZaAIq1NKmKqujKCYXKxKrFqKjKogZKAg9Kx2waxUmKqKjKxElKotmFxvla3e4KzUrK6KXK4UmFxiCMoU9KrudpoivaxpCFqblpAEZMm==')\n","    data = u.read()\n","    u.close()\n","    \n","else:\n","    print('Data has already been downloaded and extracted.')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Etgn2vm3phM4","colab_type":"text"},"source":["### Load dataset"]},{"cell_type":"code","metadata":{"id":"wBxkj9UsphM5","colab_type":"code","colab":{}},"source":["speech_data = np.load(\"/content/drive/My Drive/datasets/speech_wav_8000.npz\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QWAbi_w0phM9","colab_type":"text"},"source":["### Model dataset setting\n","* 변환된 데이터를 이용해서 학습에 활용할 데이터셋을 설정한다."]},{"cell_type":"code","metadata":{"id":"Kl5pQr-WphM-","colab_type":"code","colab":{}},"source":["sr = 8000\n","train_wav, test_wav, train_label, test_label = train_test_split(speech_data[\"wav_vals\"], \n","                                                                speech_data[\"label_vals\"], \n","                                                                test_size=0.1,\n","                                                                shuffle=True)\n","\n","train_wav = train_wav.reshape(-1,sr,1)\n","test_wav = test_wav.reshape(-1,sr,1)\n","\n","print(train_wav.shape)\n","print(test_wav.shape)\n","print(train_label.shape)\n","print(test_label.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jVsnK_FaphNC","colab_type":"code","colab":{}},"source":["target_list = ['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go']\n","\n","label_value = target_list\n","label_value.append('unknown')\n","label_value.append('silence')\n","\n","new_label_value = dict()\n","for i, l in enumerate(label_value):\n","    new_label_value[l] = i\n","label_value = new_label_value"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"L0aAi4F_phNF","colab_type":"code","colab":{}},"source":["temp = []\n","for v in train_label:\n","    temp.append(label_value[v[0]])\n","train_label = np.array(temp)\n","\n","temp = []\n","for v in test_label:\n","    temp.append(label_value[v[0]])\n","test_label = np.array(temp)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"57Pwj0sLphNI","colab_type":"code","colab":{}},"source":["print('Train_Wav Demension : ' + str(np.shape(train_wav)))\n","print('Train_Label Demension : ' + str(np.shape(train_label)))\n","print('Test_Wav Demension : ' + str(np.shape(test_wav)))\n","print('Test_Label Demension : ' + str(np.shape(test_label)))\n","print('Number Of Labels : ' + str(len(label_value)))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oqZX6VQr2sZS","colab_type":"text"},"source":["### Hyper-parameters setting"]},{"cell_type":"code","metadata":{"id":"UeeGM0Mi2v7i","colab_type":"code","colab":{}},"source":["batch_size = #\n","max_epochs = #\n","\n","# the save point\n","if use_colab:\n","    checkpoint_dir ='./drive/My Drive/train_ckpt/wave/exp1'\n","    if not os.path.isdir(checkpoint_dir):\n","        os.makedirs(checkpoint_dir)\n","else:\n","    checkpoint_dir = 'wave/exp1'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zA4D6kmyphNM","colab_type":"code","colab":{}},"source":["def one_hot_label(wav, label):\n","    label = tf.one_hot(label, depth=12)\n","    return wav, label"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"50FegMTNphNQ","colab_type":"code","colab":{}},"source":["# for train\n","train_dataset = #\n","train_dataset = train_dataset.map(one_hot_label)\n","train_dataset = train_dataset.repeat().batch(batch_size=batch_size)\n","print(train_dataset)\n","\n","# for test\n","test_dataset = #\n","test_dataset = test_dataset.map(one_hot_label)\n","test_dataset = test_dataset.batch(batch_size=batch_size)\n","print(test_dataset)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HVlQ7tGRkZwc","colab_type":"text"},"source":["### Model 구현\n","* Wave 파일 데이터를 이용해 학습을 할 수 있는 모델을 구현합니다."]},{"cell_type":"code","metadata":{"id":"imYxkRlyphNT","colab_type":"code","colab":{}},"source":["input_tensor = layers.Input(shape=(sr, 1))\n","\n","x = #\n","\n","output_tensor = layers.Dense(len(label_value), activation='softmax')(x)\n","\n","model = tf.keras.Model(input_tensor, output_tensor)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"igiurfQXphNY","colab_type":"code","colab":{}},"source":["optimizer=#\n","model.compile(loss=#,\n","             optimizer=optimizer,\n","             metrics=['accuracy'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4_DpuDp3phNb","colab_type":"code","colab":{}},"source":["# without training, just inference a model in eager execution:\n","predictions = model(train_wav[0:1], training=False)\n","print(\"Predictions: \", predictions.numpy())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U83sWOlfIe6O","colab_type":"text"},"source":["### 구성된 모델 확인"]},{"cell_type":"code","metadata":{"id":"3PhoJx8wphNe","colab_type":"code","colab":{}},"source":["model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wJkpthtZIigi","colab_type":"text"},"source":["### 모델 학습"]},{"cell_type":"code","metadata":{"id":"KygmTczhphNg","colab_type":"code","colab":{}},"source":["# using `tf.data.Dataset`\n","history = model.fit(#train_data,\n","                    epochs=#,\n","                    steps_per_epoch=#,\n","                    validation_data=#,\n","                    validation_steps=#,\n","                    )"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NaLXC0aKphNj","colab_type":"code","colab":{}},"source":["acc = history.history['accuracy']\n","val_acc = history.history['val_accuracy']\n","\n","loss=history.history['loss']\n","val_loss=history.history['val_loss']\n","\n","epochs_range = range(max_epochs)\n","\n","plt.figure(figsize=(8, 8))\n","plt.subplot(1, 2, 1)\n","plt.plot(epochs_range, acc, label='Training Accuracy')\n","plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n","plt.legend(loc='lower right')\n","plt.title('Training and Validation Accuracy')\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(epochs_range, loss, label='Training Loss')\n","plt.plot(epochs_range, val_loss, label='Validation Loss')\n","plt.legend(loc='upper right')\n","plt.title('Training and Validation Loss')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mGn-bvDej1Wz","colab_type":"text"},"source":["## Evaluation\n","* Test dataset을 이용해서 모델의 성능을 평가합니다."]},{"cell_type":"code","metadata":{"id":"6VonCCKaphNm","colab_type":"code","colab":{}},"source":["results = model.evaluate(test_dataset)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fJ6TU2s36-To","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}